{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c026b3-022a-4eaf-b833-33f82368f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.18.1\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "print(IPython.__version__)  # Check your IPython version\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97eae217-36f1-4783-b7cb-fc283e66470e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/deep-learning-env/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/deep-learning-env/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <4C793A59-B32A-3AF1-BEA5-03AD7C5C80C6> /opt/anaconda3/envs/deep-learning-env/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/deep-learning-env/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/deep-learning-env/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/deep-learning-env/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/deep-learning-env/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.6.0.dev20241112\n",
      "transformers version: 4.46.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoConfig\n",
    "import json\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import lm_eval\n",
    "from lm_eval import evaluator, tasks, utils\n",
    "from lm_eval.api.model import LM\n",
    "from lm_eval.api.registry import register_model\n",
    "from lm_eval.tasks import get_task_dict\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import ftfy\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from itertools import zip_longest\n",
    "\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85e5e86b-c3b0-4218-bdb3-e667a493cb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('../')\n",
    "sys.path.append('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411fa535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.config import BenchmarkConfig, DatasetConfig\n",
    "from data.data import DatasetManager, ModelDataset\n",
    "from data.metrics import BenchmarkMetrics\n",
    "from models.h2o.h2o_gptneox import GPTNeoXAttention_Mask, convert_kvcache_gpt_neox_heavy_recent\n",
    "from models.h2o.h2o_llama import LlamaAttention_heavy_hitter, convert_kvcache_llama_heavy_recent\n",
    "from models.h2o.h2o_opt import OPTAttention_Mask, convert_kvcache_opt_heavy_recent\n",
    "from models.base_models import ModelLoader\n",
    "from scripts.run_benchmark import run_single_strategy_benchmark\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50879937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddfb1ae4-94f1-4261-ba42-7caa82d1398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = BenchmarkConfig(\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v0.1\",\n",
    "    model_type=\"llama\",\n",
    "    device=\"mps\",\n",
    "    sequence_length=256,\n",
    "    max_tokens=32,\n",
    "    temperature=0.7,\n",
    "    datasets=[\n",
    "        DatasetConfig(\n",
    "            name=\"super_glue\",\n",
    "            config=\"copa\",\n",
    "            splits=[\"test[:10]\"],\n",
    "            input_prefix=\"Question: \",\n",
    "            output_prefix=\"Answer: \"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25fbebf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing full strategy with 100% cache\n",
      "Cleaning up memory...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f3b1b7ceb24329b59839d4b6a22d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/652 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9dc40af33d407781b56b1d134a91e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b4cc4d95c24b748d2c736a810a7b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/63.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6ec172598e4f29b14e331ddf8f8628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab16b6155ee46e0b0a6a953af669b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54604db2224f4014a33bc9d5d01dbd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbd9a9a7cd1424d8b420b25b2245879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6d77f5738449959fc9fb535227f90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thhuang/llm-training/liquid-fusion/scripts/../scripts/run_benchmark.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/opt/anaconda3/envs/deep-learning-env/lib/python3.9/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading super_glue dataset (copa) (test[:10] split) with batch_size=1...\n",
      "Dataset cached! Size: 10 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/10 [00:00<?, ?it/s]/Users/thhuang/llm-training/liquid-fusion/scripts/../scripts/run_benchmark.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "Processing examples:   0%|          | 0/10 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to benchmark_results/full_TinyLlama-1.1B-Chat-v0.1_cache100_20241208_001421.json\n",
      "Cleaning up memory...\n"
     ]
    }
   ],
   "source": [
    "full_config = copy.deepcopy(base_config)\n",
    "full_config.attention_type = \"default\"\n",
    "run_single_strategy_benchmark(full_config, strategy=\"full\", cache_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed235f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d686fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_single_strategy_benchmark(\n",
    "    strategy_config, \n",
    "    strategy=strategy, \n",
    "    cache_size=cache_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1221d2b-5a7c-4a71-8354-f83e679f8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base configuration template\n",
    "base_config = BenchmarkConfig(\n",
    "    model_name=\"huggyllama/llama-7b\",\n",
    "    model_type=\"llama\",\n",
    "    device=\"cuda\",\n",
    "    sequence_length=256,\n",
    "    max_tokens=32,\n",
    "    temperature=0.7,\n",
    "    datasets=[\n",
    "        DatasetConfig(\n",
    "            name=\"super_glue\",\n",
    "            config=\"copa\",\n",
    "            splits=[\"test\"],\n",
    "            input_prefix=\"Question: \",\n",
    "            output_prefix=\"Answer: \"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "# Full attention (baseline)\n",
    "full_config = copy.deepcopy(base_config)\n",
    "full_config.attention_type = \"default\"\n",
    "run_single_strategy_benchmark(full_config, strategy=\"full\", cache_size=100)\n",
    "\n",
    "# H2O (Heavy-Hitter + Recent)\n",
    "h2o_config = copy.deepcopy(base_config)\n",
    "h2o_config.attention_type = \"h2o\"\n",
    "h2o_config.heavy_ratio = 0.1\n",
    "h2o_config.recent_ratio = 0.1\n",
    "run_single_strategy_benchmark(h2o_config, strategy=\"h2o\", cache_size=100)\n",
    "run_single_strategy_benchmark(h2o_config, strategy=\"h2o\", cache_size=80)\n",
    "run_single_strategy_benchmark(h2o_config, strategy=\"h2o\", cache_size=40)\n",
    "run_single_strategy_benchmark(h2o_config, strategy=\"h2o\", cache_size=20)\n",
    "run_single_strategy_benchmark(h2o_config, strategy=\"h2o\", cache_size=4)\n",
    "\n",
    "# # Streaming Attention\n",
    "streaming_config = copy.deepcopy(base_config)\n",
    "streaming_config.attention_type = \"streaming\"\n",
    "streaming_config.window_size = 64\n",
    "streaming_config.sink_size = 4\n",
    "streaming_config.sink_update_rate = 0.1\n",
    "run_single_strategy_benchmark(streaming_config, strategy=\"streaming\", cache_size=100)\n",
    "run_single_strategy_benchmark(streaming_config, strategy=\"streaming\", cache_size=80)\n",
    "run_single_strategy_benchmark(streaming_config, strategy=\"streaming\", cache_size=40)\n",
    "run_single_strategy_benchmark(streaming_config, strategy=\"streaming\", cache_size=20)\n",
    "run_single_strategy_benchmark(streaming_config, strategy=\"streaming\", cache_size=4)\n",
    "\n",
    "\n",
    "# # Local/Fixed Window\n",
    "local_config = copy.deepcopy(base_config)\n",
    "local_config.attention_type = \"local\"\n",
    "local_config.window_size = 64\n",
    "run_single_strategy_benchmark(local_config, strategy=\"local\", cache_size=100)\n",
    "run_single_strategy_benchmark(local_config, strategy=\"local\", cache_size=80)\n",
    "run_single_strategy_benchmark(local_config, strategy=\"local\", cache_size=40)\n",
    "run_single_strategy_benchmark(local_config, strategy=\"local\", cache_size=20)\n",
    "run_single_strategy_benchmark(local_config, strategy=\"local\", cache_size=4)\n",
    "\n",
    "# # Liquid Fusion\n",
    "liquid_config = copy.deepcopy(base_config)\n",
    "liquid_config.attention_type = \"liquid_fusion\"\n",
    "liquid_config.window_size = 64\n",
    "liquid_config.sink_size = 2\n",
    "liquid_config.sink_update_rate = 0.1\n",
    "liquid_config.heavy_ratio = 0.1\n",
    "liquid_config.recent_ratio = 0.1\n",
    "run_single_strategy_benchmark(liquid_config, strategy=\"liquid_fusion\", cache_size=100)\n",
    "run_single_strategy_benchmark(liquid_config, strategy=\"liquid_fusion\", cache_size=80)\n",
    "run_single_strategy_benchmark(liquid_config, strategy=\"liquid_fusion\", cache_size=40)\n",
    "run_single_strategy_benchmark(liquid_config, strategy=\"liquid_fusion\", cache_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d1f911c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing liquid_fusion strategy with 20% cache\n",
      "Cleaning up memory...\n",
      "Converting to liquid_fusion attention...\n",
      "Error: The size of tensor a (2048) must match the size of tensor b (256) at non-singleton dimension 0\n",
      "Cleaning up memory...\n"
     ]
    }
   ],
   "source": [
    "liquid_config = copy.deepcopy(base_config)\n",
    "liquid_config.attention_type = \"liquid_fusion\"\n",
    "liquid_config.window_size = 64\n",
    "liquid_config.sink_size = 2\n",
    "liquid_config.sink_update_rate = 0.1\n",
    "liquid_config.heavy_ratio = 0.1\n",
    "liquid_config.recent_ratio = 0.1\n",
    "# run_single_strategy_benchmark(liquid_config, strategy=\"liquid_fusion\", cache_size=100)\n",
    "# run_single_strategy_benchmark(liquid_config, strategy=\"liquid_fusion\", cache_size=80)\n",
    "# run_single_strategy_benchmark(liquid_config, strategy=\"liquid_fusion\", cache_size=40)\n",
    "run_single_strategy_benchmark(liquid_config, strategy=\"liquid_fusion\", cache_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49529e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08:01:29:47,121 INFO     [2674569922.py:40] Configuration created:\n",
      "2024-12-08:01:29:47,122 INFO     [2674569922.py:41] Model: TinyLlama/TinyLlama-1.1B-Chat-v0.1\n",
      "2024-12-08:01:29:47,122 INFO     [2674569922.py:42] Sequence length: 256\n",
      "2024-12-08:01:29:47,123 INFO     [2674569922.py:43] Max position embeddings: 256\n",
      "2024-12-08:01:29:47,123 INFO     [2674569922.py:44] Window size: 64\n",
      "2024-12-08:01:29:47,123 INFO     [2674569922.py:48] Starting benchmark with cache_size=20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing liquid_fusion strategy with 20% cache\n",
      "Cleaning up memory...\n",
      "Converting to liquid_fusion attention...\n",
      "Loading super_glue dataset (copa) (test split) with batch_size=1...\n",
      "Dataset cached! Size: 500 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/500 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to benchmark_results/liquid_fusion_TinyLlama-1.1B-Chat-v0.1_cache20_20241208_012957.json\n",
      "Cleaning up memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08:01:29:58,101 INFO     [2674569922.py:50] Benchmark completed successfully\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import copy\n",
    "from data.config import BenchmarkConfig, DatasetConfig\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def test_liquid_fusion():\n",
    "    # Base configuration with explicit position embeddings\n",
    "    base_config = BenchmarkConfig(\n",
    "        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v0.1\",\n",
    "        model_type=\"llama\",\n",
    "        device=\"mps\",\n",
    "        sequence_length=256,  # Match TinyLlama's architecture\n",
    "        max_tokens=32,\n",
    "        temperature=0.7,\n",
    "        max_position_embeddings=256,  # TinyLlama's default\n",
    "        datasets=[\n",
    "            DatasetConfig(\n",
    "                name=\"super_glue\",\n",
    "                config=\"copa\",\n",
    "                splits=[\"test\"],\n",
    "                input_prefix=\"Question: \",\n",
    "                output_prefix=\"Answer: \",\n",
    "                max_samples=5\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Configure Liquid Fusion with matching dimensions\n",
    "    liquid_config = copy.deepcopy(base_config)\n",
    "    liquid_config.attention_type = \"liquid_fusion\"\n",
    "    liquid_config.window_size = 64\n",
    "    liquid_config.sink_size = 2\n",
    "    liquid_config.sink_update_rate = 0.1\n",
    "    liquid_config.heavy_ratio = 0.1\n",
    "    liquid_config.recent_ratio = 0.1\n",
    "    \n",
    "    logger.info(\"Configuration created:\")\n",
    "    logger.info(f\"Model: {liquid_config.model_name}\")\n",
    "    logger.info(f\"Sequence length: {liquid_config.sequence_length}\")\n",
    "    logger.info(f\"Max position embeddings: {liquid_config.max_position_embeddings}\")\n",
    "    logger.info(f\"Window size: {liquid_config.window_size}\")\n",
    "    \n",
    "    try:\n",
    "        from scripts.run_benchmark import run_single_strategy_benchmark\n",
    "        logger.info(\"Starting benchmark with cache_size=20\")\n",
    "        result = run_single_strategy_benchmark(liquid_config, strategy=\"liquid_fusion\", cache_size=20)\n",
    "        logger.info(\"Benchmark completed successfully\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during benchmark: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_liquid_fusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67772712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
